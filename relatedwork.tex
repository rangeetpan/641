\section{Related Work}
\label{sec:relatedwork}
\subsection{\textbf{Study on accountable ML model.}}
\paragraph{Validating Accountability}
There is a vast amount of research on validating and holding an ML model accountable. One of the earliest validation technique proposed by Pulina and Tacchella \cite{pulina2010abstraction} that utilizes abstraction to explain special types of a neural network, multi-layer perceptrons (MLP) and proposed a refinement approach to study the safety of MLP and repair them. However, this work can only be utilized if a network has six neurons. Gehr et al. \cite{gehr2018ai2} has proposed an infrastructure $AI^{2}$ that converts a neural network with convolution and fully connected layers to address the safety and robustness of the ML models. This work has addressed some issues, e.g., the trade-off between precision and scalability, presenting an abstract representation of robustness in convolutional operation. However, this infrastructure does not work for a complex model, and it suffers from scalability issues. \cite{du2018techniques} has surveyed models and research papers and have found that though there are a vast majority of work in machine learning has been done on increasing the explainability of the models, these researches do not answer some question developers have e.g., \emph{"why Q, not R"} or in our case why the accuracy changes with the same experimental condition and how we can believe an ML model if it does not provide a concrete contract to the end-user. Similar survey has been conducted by \cite{abdul2018trends} that studied vast amount research papers and have concluded the overall theme for HCI researchers to hold an ML model accountable.

Interpretable machine learning is an active area of research, with numerous interpretations and accountable approaches evolving recently. A comprehensive survey \cite{du2018techniques} describes the clear categorization and complete overview of prevalent techniques to increase the interpretability of machine learning models aiming to help the community to understand the capabilities and weaknesses of different accountable approaches better.


\paragraph{Increasing accountability}
\cite{jia2019taso} proposed a programming language to prune the neural network to explain and interpret the model behavior and find bugs in the graph-based operations. RELUVAL \cite{wang2018formal} proposed a symbolic interval analysis to provide a formal guarantee of a deep neural network-based model. This work has proposed a technique to formalize the dependency information of a network while the network operations propagate. Input dependency and output estimation related problems are addressed by providing an interval that is similar to our work where we demonstrate the output accuracy in terms of an interval based on the input dependencies propagating through the network. Similar to the RELUVAL, \cite{katz2017reluplex} built a framework based on SMT solver to verify neural networks.

There is another significant research study \cite{lai2018human} to investigate whether machine predictions and their explanations can improve human performance in challenging tasks such as deception detection. They discovered that without demonstrating predicted labels, only interpretations slightly improve human performance in the end task. Finally, their obtained results reveal a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff. Another research study \cite{yang2019learn} discusses what makes an explanation, so to say interpretation, and suggests that machine learning might benefit from viewing the problem more broadly.

Another area of research includes the robustness increase through crafting and creating defense against adversarial attack \cite{papernot2016towards}  that posses a threat against an ML model. \cite{anderson2019optimization,pan2019static} produces verification process to defense against such attacks.

%\cite{abdul2018trends} studied 289 core papers on explanations and explainable systems, as well as 12,412 citing papers to investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of. Abdul et al. use topic modeling, co-occurrence, and network analysis to identify fading and burgeoning trends in explainable systems and identify domains that are closely connected or mostly isolated. Zhang et al. \cite{zhang2016understanding} show the reason why the traditional approaches cannot explain how a large neural network work by using extensive systematic experiments. Moreover, by using these experiment, the author also establishes that the image classification convolutional, which is trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization and occurs even if we replace the true images by completely unstructured random noise. \cite{adebayo2018sanity} shows that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. SoK \cite{papernot2016towards} introduces a unified threat model that considers the attack surface and adversarial goals and capabilities of systems built on machine learning. The security model supports the authors in exploring attacks and defenses of ML systems. Explainable AI \cite{goebel2018explainable} create extremely important verbal explanations are for the emerging field of "explainable artificial intelligence," which opens additional application fields. Greg et al. \cite{anderson2019optimization} introduce an algorithm for verifying the robustness properties of neural networks. Specifically, their method combines the best of proof-based and optimization-based methods to obtain a sound and complete decision procedure. 




%\textbf{}
\subsection{\textbf{Accuracy validation}}
The interpretability of a ML model output provides significant insights into the ML model, which can be used to transform an untrustworthy model or prediction into a trustworthy, e.g., accountable one. Ribeiro et al. \cite{ribeiro2016should} focused on this and proposed LIME, a modular and extensible approach to faithfully explain the predictions of any model in an accountable manner. They also introduced SP-LIME, another method to choose representative and non-redundant predictions, providing a global view of the machine learning model to users.

In another study \cite{du2018towards}, a class-discriminative DNN interpretation model has been proposed to explain why a DNN classifier makes a specific prediction for an instance. It has been demonstrated that the inner representations of DNNs provide a tool to interpret and diagnose the working mechanism of individual predictions. The experimental results also validate that the proposed guided feature inversion method performs well in preserving the information of all crucial foreground objects, regardless of their category.

In another study \cite{du2018towards}, a class-discriminative DNN interpretation model has been proposed to explain why a DNN classifier makes a specific prediction. It has been demonstrated that the inner representations of DNNs provide a tool to interpret and diagnose the working mechanism of individual predictions. The experimental results also validate that the proposed guided feature inversion method performs well in preserving the information of all crucial foreground objects, regardless of their category.
