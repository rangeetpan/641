\section{Related Work}
\label{sec:relatedwork}
\paragraph{\textbf{Study on accountable ML model.}}
There is a vast amount of research on validating and holding an ML model. One of the earliest validation technique proposed by Pulina and Tacchella \cite{pulina2010abstraction} that utilizes abstraction to explain special types of a neural network, multi-layer perceptrons (MLP) and proposed a refinement approach to study the safety of MLP and repair them. However, this work can only be utilized if a network has six neurons. Gehr et al. \cite{gehr2018ai2} has proposed an infrastructure $AI^{2}$ that converts a neural network with convolution and fully connected layers to address the safety and robustness of the ML models. This work has address some issue e.g., the trade-off between precision and scalability, presenting an abstract representation of robustness in convolutional operation. However, this infrastructure does not work for a complex model and it suffers from scalability issues. RELUVAL \cite{wang2018formal} proposed a symbolic interval analysis to provide a formal guarantee of a deep neural network-based model. This work has proposed a technique to formalize the dependency information of a network while the network operations propagate. Input dependency and output estimation related problems are addressed by providing an interval that is similar to our work where we demonstrate the output accuracy in terms of an interval based on the input dependencies propagating through the network. Similar to the RELUVAL, \cite{katz2017reluplex} built a framework based on SMT solver to verify neural networks. \cite{jia2019taso} proposed a programming language to prune the neural network to explain and interpret the model behavior and find bugs in the graph-based operations. \cite{du2018techniques} has surveyed models and research papers and have found that though there are a vast majority of work in machine learning has been done on increasing the explainability of the models, these researches do not answer some question developers have e.g., \emph{"why Q, not R"} or in our case why the accuracy changes with the same experimental condition and how we can believe an ML model if it simply does not provide a contract to the end-user. \cite{abdul2018trends} studied 289 core papers on explanations and explainable systems, as well as 12,412 citing papers to investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of. Abdul et al. use topic modeling, co-occurrence and network analysis to identify fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. Zhang et al. \cite{zhang2016understanding} show the reason why the traditional approaches cannot explain how a large neural network work by using extensive systematic experiments. Moreover, by using these experiment, the author also establish that  the image classification convolutional which is trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. \cite{adebayo2018sanity} shows that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. SoK \cite{papernot2016towards} introduces a unified threat model that considers the attack surface and adversarial goals and capabilities of systems built on machine learning. The security model support the authors in exploring attacks and defenses of ML systems. Explainable AI \cite{goebel2018explainable} create an extremely important verbal explanations are  for the emerging field of "explainable artificial intelligence", which opens additional application fields. Greg et al. \cite{anderson2019optimization} introduce an algorithem for verifying robustness properties of neural networks. Specifically, their method combines the best of proof based and optimization-based methods to obtain a sound and complete decision procedure. 

Interpretable machine learning is an active area of research, with  numerous interpretation and accountable approaches evolving recently. A comprehensive survey [duplicate] describes the clear categorization and complete overview of prevalent techniques to increase the interpretability of machine learning models aiming to help the community to better understand the capabilities and weaknesses of different accountable approaches.

There is another significant research study [2] to investigate whether machine predictions and their explanations can improve human performance in challenging tasks such as deception detection. They discovered that without demonstrating predicted labels, only interpretations  slightly improve human performance in the end task. Finally their obtained results reveal a tradeoff between human performance and human agency and show that explanations of machine predictions can moderate this tradeoff. Another research study [4] discusses on what makes an explanation so to say interpretation, and suggests that machine learning might benefit from viewing the problem more broadly.
%\textbf{}
\paragraph{\textbf{Accuracy validation}}
Interpretability of ML model output provides significant insights into the ML model, which can be used to transform an untrustworthy model or prediction into a trustworthy i.e., accountable one. Ribeiro et al. [5] focused on this and proposed LIME, a modular and extensible approach to faithfully explain the predictions of any model in an accountable manner. They also introduced SP-LIME, another method to choose representative and non-redundant predictions, providing a global view of the machine learning model to users.

In another study [7], a class-discriminative DNN interpretation model has been proposed to explain why a DNN classifier makes a specific prediction for an instance. It has been demonstrated that the inner representations of DNNs provide a tool to interpret and diagnose the working mechanism of individual predictions. The experimental results also validate that the proposed guided feature inversion method performs well in preserving the information of all crucial foreground objects, regardless of their category.

In another significant research study [6], a general method to modify traditional CNNs to enhance their interpretability has been proposed. Experiments have shown that the proposed interpretable CNNs encoded more semantically meaningful knowledge in high conv-layers than existing CNNs. In this case, the CNNâ€™s classification accuracy may decrease a bit because when an interpretable CNN has been deployed to classify a large number of categories simultaneously, filters in a conv-layer are assigned with different categories, which makes each category corresponds to only a few filters. So, accuracy validation is crucial while making interpretable CNN.
