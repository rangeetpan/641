\section{Related Work}
\label{sec:relatedwork}
\paragraph{\textbf{Study on accountable ML model.}}
There is a vast amount of research on validating and holding an ML model. One of the earliest validation technique proposed by Pulina and Tacchella \cite{pulina2010abstraction} that utilizes abstraction to explain special types of a neural network, multi-layer perceptrons (MLP) and proposed a refinement approach to study the safety of MLP and repair them. However, this work can only be utilized if a network has six neurons. Gehr et al. \cite{gehr2018ai2} has proposed an infrastructure $AI^{2}$ that converts a neural network with convolution and fully connected layers to address the safety and robustness of the ML models. This work has address some issue e.g., the trade-off between precision and scalability, presenting an abstract representation of robustness in convolutional operation. However, this infrastructure does not work for a complex model and it suffers from scalability issues. RELUVAL \cite{wang2018formal} proposed a symbolic interval analysis to provide a formal guarantee of a deep neural network-based model. This work has proposed a technique to formalize the dependency information of a network while the network operations propagate. Input dependency and output estimation related problems are addressed by providing an interval that is similar to our work where we demonstrate the output accuracy in terms of an interval based on the input dependencies propagating through the network. Similar to the RELUVAL, \cite{katz2017reluplex} built a framework based on SMT solver to verify neural networks. \cite{jia2019taso} proposed a programming language to prune the neural network to explain and interpret the model behavior and find bugs in the graph-based operations. \cite{du2018techniques} has surveyed models and research papers and have found that though there are a vast majority of work in machine learning has been done on increasing the explainability of the models, these researches do not answer some question developers have e.g., \emph{"why Q, not R"} or in our case why the accuracy changes with the same experimental condition and how we can believe an ML model if it simply does not provide a contract to the end-user. \cite{abdul2018trends} studied 289 core papers on explanations and explainable systems, as well as 12,412 citing papers to investigate how HCI researchers can help to develop accountable systems by performing a literature analysis of. Abdul et al. use topic modeling, co-occurrence and network analysis to identify fading and burgeoning trends in explainable systems, and identify domains that are closely connected or mostly isolated. Zhang et al. \cite{zhang2016understanding} show the reason why the traditional approaches cannot explain how a large neural network work by using extensive systematic experiments. Moreover, by using these experiment, the author also establish that  the image classification convolutional which is trained with stochastic gradient methods easily fit a random labeling of the training data. This phenomenon is qualitatively unaffected by explicit regularization, and occurs even if we replace the true images by completely unstructured random noise. \cite{adebayo2018sanity} shows that some existing saliency methods are independent both of the model and of the data generating process. Consequently, methods that fail the proposed tests are inadequate for tasks that are sensitive to either data or model, such as, finding outliers in the data, explaining the relationship between inputs and outputs that the model learned, and debugging the model. SoK \cite{papernot2016towards} introduces a unified threat model that considers the attack surface and adversarial goals and capabilities of systems built on machine learning. The security model support the authors in exploring attacks and defenses of ML systems. Explainable AI \cite{goebel2018explainable} create an extremely important verbal explanations are  for the emerging field of "explainable artificial intelligence", which opens additional application fields. Greg et al. \cite{anderson2019optimization} introduce an algorithem for verifying robustness properties of neural networks. Specifically, their method combines the best of proof based and optimization-based methods to obtain a sound and complete decision procedure.  

\textbf{}
\paragraph{\textbf{Accuracy validation}}
