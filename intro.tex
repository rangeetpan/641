% $Id: intro.tex,v 1.1.1.1 2007/10/03 22:28:18 hridesh Exp $

\section{Introduction}
With the increasing popularity of Machine Learning (ML) based systems, the verification and validation of such systems are in need. Though there is a vast amount of research carried out on the non-ML validation framework; however, due to the nature of probabilistic and complexity, there is a few validation based research done on the ML-based systems. Without a proper validation framework, one might ask about the way to hold ML-based systems being accountable to the responsibility imposed on it. One such responsibility is the contract made with the end-user by exposing the trustworthiness of these systems in terms of accuracy. However, this metric of trustworthiness is variant even if the whole experimental setup remains the same. In this study, we address such problems and have proposed a programming language infrastructure to measure the accuracy in a closed range.

Aproach: While there are works on holding an ML-based system accountable for the assigned task, these works are primarily categorized into two divisions, accountability validation and increasing the explainability of such systems. Our approach is randomly picking seed values, bias, and weight to verify the deep neural network (DNN). Seed value ... . When the inputs are transmitted between neurons, the weights are applied to the inputs and passed into an activation function along with the bias. With different set of input, we can obtain different outputs which can be accuracy or loss values. Therefore, After multiple times modifying this input set, we will obtain a range of output values. In a specific input set, if the output value is far from the output range acquired by using previous input sets, we can detect the erro

Results: 

Conclusions: 






The prior works have focused on validating the input influence, explaining the models to make the black-box system grayer. However, these systems either hold domain knowledge or model operation knowledge as the key to increase the explainability. In this study, we have combined these two type of knowledge and propose a system that can takes the input dataset, model operations and their distributions as input and produce a metric of accuracy in a closed range rather than a single value that changes everytime a model has been trained with same dataset and same experimental conditions.

Our contribution to this study has been the following:
\begin{itemize}
	\item We have proposed a framework that takes the domain and ML-based operation knowledge to understand the system as a whole.
	\item We have proposed a programming language that can validate the trustworthiness of an ML-based system in a closed range.
\end{itemize}
