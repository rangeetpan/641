% $Id: intro.tex,v 1.1.1.1 2007/10/03 22:28:18 hridesh Exp $

\section{Introduction}
With the increasing popularity of Machine Learning (ML) based systems, the verification and validation of such systems are in need. Though there is a vast amount of research carried out on the non-ML validation framework; however, due to the nature of probabilistic and complexity, there is a few validation based research done on the ML-based systems. Without a proper validation framework, one might ask about the way to hold ML-based systems being accountable to the responsibility imposed on it. One such responsibility is the contract made with the end-user by exposing the trustworthiness of these systems in terms of accuracy. However, this metric of trustworthiness is variant even if the whole experimental setup remains the same. In this study, we address such problems and have proposed a programming language infrastructure to measure the accuracy in a closed range.

While there are works on holding an ML-based system accountable for the assigned task, these works are primarily categorized into two divisions, accountability validation and increasing the explainability of such systems. The prior works have focused on validating the input influence, explaining the models to make the black-box system grayer. However, these systems either hold domain knowledge or model operation knowledge as the key to increase the explainability. In this study, we have combined these two type of knowledge and propose a system that can takes the input dataset, model operations and their distributions as input and produce a metric of accuracy in a closed range rather than a single value that changes everytime a model has been trained with same dataset and same experimental conditions. Our approach is obtaining the range of seed values, bias, and weight to verify the deep neural network (DNN). Specifically, the DNN randomly generate these value whenever it train. Moreover, with different set of seed value, bias, and weight, we can obtain the different output values. Therefore, if we can identify the range, from lowest value to highest value, of seed values, bias, and weight, we can acquire the range of output value. We will perform our method in multiple DNNs in the same dataset; and we can get the range of output of these DNNs. By comparing DNNs' out ranges, we can verify whether a DNN is good or bad.

If we have a trained DNN which includes model structure, seed value, bias, weight, we can have the a same accuracy for multiple training section. However, if we have only the model structure, the final accuracy can be different because each training section can create each different accuracy. Therefore, in this situation, we cannot identify which model is better just based on the accuracy. However, it can be a different story if we have a range of the accuracy. Accuracy range is not changed after we retrain the model; therefore, the range a reliable set of value if identify the quality of the DNN.

%When the inputs are transmitted between neurons, the weights are applied to the inputs and passed into an activation function along with the bias. With different set of input, we can obtain different outputs which can be accuracy or loss values. Therefore, After multiple times modifying this input set, we will obtain a range of output values. In a specific input set, if the output value is far from the output range acquired by using previous input sets, we can detect the erro

Our contribution to this study has been the following:
\begin{itemize}
	\item We have proposed a framework that takes the domain and ML-based operation knowledge to understand the system as a whole.
	\item We have proposed a programming language that can validate the trustworthiness of an ML-based system in a closed range.
\end{itemize}
